{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding linguistic patterns\n",
    "\n",
    "This section introduces you to finding linguistic patterns using spaCy.\n",
    "\n",
    "If you are unfamiliar with the linguistic annotations produced by spaCy or need to refresh your memory, revisit [Part II](../part_ii/03_basic_nlp.ipynb) before working through this section.\n",
    "\n",
    "After reading this section, you should:\n",
    "\n",
    " - 1\n",
    " - 2\n",
    " - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding patterns using spaCy Matchers\n",
    "\n",
    "spaCy provides three types of Matchers:\n",
    "\n",
    "1. A [Matcher](https://spacy.io/api/matcher), which allows defining rules that search for particular **words or phrases** by examining *Token* attributes.  \n",
    "2. A [DependencyMatcher](https://spacy.io/api/dependencymatcher), which allows searching parse trees for **syntactic patterns**.\n",
    "3. A [PhraseMatcher](https://spacy.io/api/phrasematcher), a fast method for matching *Doc* objects to *Doc* objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Matcher\n",
    "\n",
    "To get started with the *Matcher*, let's import the spaCy library and load a small language model for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the spaCy library into Python\n",
    "import spacy\n",
    "\n",
    "# Load a small language model for English; assign the result under 'nlp'\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give us some data to work with, let's load some text extracted from a Wikipedia article and process it using the language model under the variable `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14867"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the file 'occupy.txt' and use the read() method to read the contents.\n",
    "# Feed the result to the language model under 'nlp'.\n",
    "doc = nlp(open(file='data/occupy.txt', mode='r', encoding='utf-8').read())\n",
    "\n",
    "# Check the length of the Doc object, that is, how many Tokens are contained within.\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a *Doc* with nearly 15 000 *Tokens*, we can continue to import the *Matcher* class from the `matcher` submodule of spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Matcher class\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the *Matcher* class allows creating *Matcher* objects, which must be initialised by providing the vocabulary object of the language model that will be used for finding matches.\n",
    "\n",
    "This vocabulary is stored in a [*Vocab*](https://spacy.io/api/vocab) object, which is available under the attribute `vocab` of a *Language* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.matcher.matcher.Matcher at 0x161deca40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Matcher and provide model vocabulary; assign result under the variable 'matcher'\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Call the variable to examine the object\n",
    "matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a *Matcher* object, which stores the patterns to be searched for.\n",
    "\n",
    "The patterns to be matched are defined using a [specific format](https://spacy.io/api/matcher#patterns) defined in spaCy.\n",
    "\n",
    "Each pattern consists of a Python list, which is populated by dictionaries, which each define a pattern for matching a single *Token*.\n",
    "\n",
    "If you wish to match a sequence of *Tokens*, the dictionaries must follow their order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_1 = [{\"POS\": \"PRON\"}, \n",
    "             {\"POS\": \"VERB\"}]\n",
    "\n",
    "matcher.add(\"PRON+VERB\", [pattern_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_1 = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It aimed\n",
      "It formed\n",
      "We are\n",
      "it organizes\n",
      "who designed\n",
      "He wrote\n",
      "They promoted\n",
      "It refers\n",
      "they saw\n",
      "they argued\n",
      "they called\n",
      "it takes\n",
      "they called\n",
      "who comment\n",
      "them using\n",
      "they belong\n",
      "himself warned\n",
      "he said\n",
      "they think\n",
      "them gain\n",
      "they wished\n",
      "they blamed\n",
      "I support\n",
      "It showed\n",
      "who gave\n",
      "they refused\n",
      "they saw\n",
      "who caused\n",
      "We are\n",
      "who sought\n",
      "who were\n",
      "who were\n",
      "who made\n",
      "who criticized\n",
      "it returned\n",
      "its proposed\n",
      "They received\n",
      "there have\n",
      "it came\n",
      "it gained\n",
      "He claimed\n",
      "they presented\n",
      "they call\n",
      "It consists\n",
      "there were\n",
      "there was\n",
      "What started\n",
      "it is\n",
      "We are\n",
      "they began\n",
      "they perceived\n",
      "they say\n",
      "We agree\n",
      "we see\n",
      "it's\n",
      "who are\n",
      "who say\n",
      "what's\n",
      "they do\n",
      "they reflect\n",
      "He mentioned\n",
      "We regard\n",
      "who participated\n",
      "he wrote\n",
      "we have\n",
      "who dislike\n",
      "they employ\n",
      "they have\n",
      "there is\n",
      "it stall\n",
      "who emerged\n",
      "It pushes\n",
      "who called\n"
     ]
    }
   ],
   "source": [
    "for match in matches_1:\n",
    "    \n",
    "    print(doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your own concordancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
