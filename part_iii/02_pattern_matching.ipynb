{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding linguistic patterns using spaCy\n",
    "\n",
    "This section teaches you to find linguistic patterns using spaCy, a natural language processing library for Python.\n",
    "\n",
    "If you are unfamiliar with the linguistic annotations produced by spaCy or need to refresh your memory, revisit [Part II](../part_ii/03_basic_nlp.ipynb) before working through this section.\n",
    "\n",
    "After reading this section, you should:\n",
    "\n",
    " - know how to search for patterns among Tokens and their sequences\n",
    " - know how to search for patterns among morphological features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding patterns using spaCy Matchers\n",
    "\n",
    "Linguistic annotations, such as part-of-speech tags, syntactic dependencies and morphological features, help impose structure on written language. Crucially, linguistic annotations allow searching for structural patterns instead of individual words or phrases. This allows defining search patterns in a flexible way.\n",
    "\n",
    "In the spaCy library, the capability for pattern search is provided by various components named Matchers.\n",
    "\n",
    "spaCy provides three types of Matchers:\n",
    "\n",
    "1. A [Matcher](https://spacy.io/api/matcher), which allows defining rules that search for particular **words or phrases** by examining *Token* attributes.  \n",
    "2. A [DependencyMatcher](https://spacy.io/api/dependencymatcher), which allows searching parse trees for **syntactic patterns**.\n",
    "3. A [PhraseMatcher](https://spacy.io/api/phrasematcher), a fast method for matching spaCy *Doc* objects to *Doc* objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding words or phrases\n",
    "\n",
    "To get started with the *Matcher*, let's import the spaCy library and load a small language model for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the spaCy library into Python\n",
    "import spacy\n",
    "\n",
    "# Load a small language model for English; assign the result under 'nlp'\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have some data to work with, let's load some text from a Wikipedia article.\n",
    "\n",
    "To do so, we use Python's `open()` function to open the file for reading, providing the `file`, `mode` and `encoding` arguments, as instructed in [Part II](../part_ii/01_basic_text_processing.ipynb#Loading-plain-text-files-into-Python).\n",
    "\n",
    "We then call the `read()` method to read the file contents, and store the result under the variable `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the open() function to open the file for reading, followed by the\n",
    "# read() method to read the contents of the file.\n",
    "text = open(file='data/occupy.txt', mode='r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a Python string object that contains the article in plain text, which is available under the variable `text`.\n",
    "\n",
    "Next, we then feed this object to the language model under the variable `nlp` as instructed in [Part II](../part_ii/03_basic_nlp.ipynb#Performing-basic-NLP-tasks-using-spaCy).\n",
    "\n",
    "We also use Python's `len()` function to count the number of words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the string object to the language model\n",
    "doc = nlp(text)\n",
    "\n",
    "# Use the len() function to check length of the Doc object to count \n",
    "# how many Tokens are contained within.\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a spaCy *Doc* object with nearly 15 000 *Tokens*, we can continue to import the *Matcher* class from the `matcher` submodule of spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Matcher class\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the *Matcher* class from spaCy's `matcher` submodule allows creating *Matcher* objects.\n",
    "\n",
    "When creating a *Matcher* object, you must provide the vocabulary of the language model used for finding matches to the *Matcher* object.\n",
    "\n",
    "The reason for this is really rather simple: if you want to search for patterns in some language, you need to know its vocabulary first.\n",
    "\n",
    "The vocabulary of a model is stored in a [*Vocab*](https://spacy.io/api/vocab) object. The *Vocab* object can be found under the attribute `vocab` of a spaCy *Language* object, which was introduced in [Part II](../part_ii/03_basic_nlp.ipynb#Performing-basic-NLP-tasks-using-spaCy).\n",
    "\n",
    "In this case, we have the *Language* object that contains a small language model for English stored under the variable `nlp`, which means we can access its *Vocab* object by calling `nlp.vocab`.\n",
    "\n",
    "We then call the *Matcher* **class** and provide the vocabulary under `nlp.vocab` to the `vocab` argument to create a *Matcher* object. We store the resulting object under the variable `matcher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Matcher and provide model vocabulary; assign result under the variable 'matcher'\n",
    "matcher = Matcher(vocab=nlp.vocab)\n",
    "\n",
    "# Call the variable to examine the object\n",
    "matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Matcher* object is now ready to store the patterns that we want to search for.\n",
    "\n",
    "These patterns, or more specifically, *pattern rules*, are created using a [specific format](https://spacy.io/api/matcher#patterns) defined in spaCy.\n",
    "\n",
    "Each pattern consists of a Python list, which is populated by Python dictionaries. \n",
    "\n",
    "Each dictionary describes the pattern for matching a single spaCy *Token*. \n",
    "\n",
    "If you wish to match a sequence of *Tokens*, you must define multiple dictionaries within a single list, whose order follows that of the pattern to be matched.\n",
    "\n",
    "Let's start by defining a simple pattern for matching sequences of pronouns and verbs, which we store under the variable `pronoun_verb`.\n",
    "\n",
    "This pattern consists of a list, as marked by the surrounding brackets `[]`, which contains two dictionaries, marked by curly braces `{}` and separated by a comma. The key and value pairs in a dictionary are separated by a colon.\n",
    "\n",
    " - The dictionary key determines which *Token* attribute should be searched for matches. The attributes supported by the *Matcher* can be found [here](https://spacy.io/api/matcher#patterns).\n",
    "\n",
    " - The value under the dictionary key determines the specific value for the attribute.\n",
    "\n",
    "In this case, we define a pattern that searches for a sequence of two coarse part-of-speech tags (`POS`), which were introduced in [Part II](../part_ii/03_basic_nlp.ipynb#Part-of-speech-tagging), namely pronouns (`PRON`) and verbs (`VERB`).\n",
    "\n",
    "Note that both keys and values must be provided in uppercase letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list with nested dictionaries that contains the pattern to be matched\n",
    "pronoun_verb = [{'POS': 'PRON'}, {'POS': 'VERB'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the pattern using a list and dictionaries, we can add it to the *Matcher* object under the variable `matcher`.\n",
    "\n",
    "This can be achieved using `add()` method, which requires two inputs:\n",
    "\n",
    " 1. A Python string object that defines a name for the pattern. This is simply for purposes of identification.\n",
    " 2. A list containing the pattern(s) to be searched for. A single rule for matching patterns can contain multiple patterns, hence the input must be a *list of lists*, e.g. `[pattern_1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the pattern to the matcher under the name 'pronoun+verb'\n",
    "matcher.add(\"pronoun+verb\", patterns=[pronoun_verb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To search for matches the *Doc* object stored under the variable `doc`, we feed the *Doc* object to the *Matcher* and store the result under the variable `result`.\n",
    "\n",
    "We also set the optional argument `as_spans` to `True`, which instructs spaCy to return the results as *Span* objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "# Apply the Matcher to the Doc object under 'doc'; provide the argument\n",
    "# 'as_spans' and set its value to True to get Spans as output\n",
    "result = matcher(doc, as_spans=True)\n",
    "\n",
    "# Call the variable to examine the output\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a list of spaCy *Span* objects that match the requested pattern. Let's examine the first object in the list of matches in greater detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Span* object has various useful attributes, including `start` and `end`. These attributes contain the indices that indicate where in the *Doc* object the *Span* starts and finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].start, result[0].end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful attribute is `label`, which contains the name that we gave to the pattern. Let's take a closer look at this attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value stored under the `label` attribute is actually a spaCy [*Lexeme*](https://spacy.io/api/lexeme) object that corresponds to an entry in the language model's vocabulary. \n",
    "\n",
    "This *Lexeme* contains the name that we gave to the search pattern above, namely `pronoun+verb`.\n",
    "\n",
    "We can easily verify this by fetching this *Lexeme* from the *Vocab* object under `nlp.vocab` and examining its `text` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab[12298179334642351811].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information under the `label` attribute is useful for disambiguating between patterns, especially if the same *Matcher* object contains multiple different patterns, as we will see shortly below.\n",
    "\n",
    "Looking at the matches above, the pattern we defined is quite restrictive, as the pronoun and the verb must follow each other.\n",
    "\n",
    "We cannot, for example, match patterns where the verb is preceded by auxiliary verbs.\n",
    "\n",
    "spaCy allows increasing the flexibility of pattern rules using operators. These operators are defined by adding the key `OP` to the dictionary that defines a pattern for a single *Token*. spaCy supports the following operators:\n",
    "\n",
    " - `!`: Negate the pattern; the pattern can occur exactly zero times.\n",
    " - `?`: Make the pattern optional; the pattern may occur zero or one times.\n",
    " - `+`: Require the pattern to occur one or more times.\n",
    " - `*`: Allow the pattern to match zero or more times.\n",
    "\n",
    "Let's explore the use of operators by defining another pattern rule, which extends the scope of our *Matcher*.\n",
    "\n",
    "To do so, we define another pattern for a *Token* between the pronoun and the verb. This *Token* must have the coarse part-of-speech tag `AUX`, which indicates an auxiliary verb. \n",
    "\n",
    "In addition, we add another key and value pair to the dictionary for this *Token*, which contains the key `OP` with the value `+`. This means that the *Token* corresponding to an auxiliary verb must occur *one or more times*.\n",
    "\n",
    "We store the resulting list with nested dictionaries under the variable `pronoun_aux_verb`, and add the pattern to the existing *Matcher* object stored under the variable `matcher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list with nested dictionaries that contains the pattern to be matched\n",
    "pronoun_aux_verb = [{'POS': 'PRON'}, {'POS': 'AUX', 'OP': '+'}, {'POS': 'VERB'}]\n",
    "\n",
    "# Add the pattern to the matcher under the name 'pronoun+aux+verb'\n",
    "matcher.add('pronoun+aux+verb', patterns=[pronoun_aux_verb])\n",
    "\n",
    "# Apply the Matcher to the Doc object under 'doc'; provide the argument 'as_spans'\n",
    "# and set its value to True to get Spans as output. Overwrite previous matches by\n",
    "# storing the result under the variable 'results'.\n",
    "results = matcher(doc, as_spans=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as above, the *Matcher* returns a list of spaCy *Span* objects.\n",
    "\n",
    "Let's loop over each item in the list `results`. We use the variable `result` to refer to the *Span* objects in the list, which contain our matches.\n",
    "\n",
    "We first retrieve the *Lexeme* object stored under `result.label`, which we map to the language model's *Vocabulary* under `nlp.vocab`. \n",
    "\n",
    "As we learned above, this *Lexeme* corresponds to the name that we gave to the pattern rule, whose human-readable form can be found under the attribute `text`.\n",
    "\n",
    "We then print a tabulator character to insert some space between the name of the pattern and the *Span* object containing the match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "# Loop over each Span object in the list 'results'\n",
    "for result in results:\n",
    "    \n",
    "    # Print out the the name of the pattern rule, a tabulator character, and the matching Span\n",
    "    print(nlp.vocab[result.label].text, '\\t', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that the pattern we added to the *Matcher* matches patterns that contain one (e.g. \"we *can* build\") or more (e.g. \"they *have been* protesting\") auxiliaries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding morphological features\n",
    "\n",
    "As introduced in [Part II](../part_ii/03_basic_nlp.ipynb#Morphological-analysis), spaCy can also perform morphological analysis for individual *Tokens*, whose results are stored under the attribute `morph` of a *Token* object.\n",
    "\n",
    "The `morph` attribute contains a string object, in which each morphological feature is separated by a vertical bar `|`, as illustrated below.\n",
    "\n",
    "```\n",
    "We \t Case=Nom|Number=Plur|Person=1|PronType=Prs\n",
    "```\n",
    "\n",
    "As you can see, particular types of morphological features, e.g. *Case*, and their type, e.g. *Nom* (for the nominative case) are separated by equal signs `=`.\n",
    "\n",
    "Let's begin exploring how we can define pattern rules that match morphological features.\n",
    "\n",
    "To get started, we create a new *Matcher* object named `morph_matcher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Matcher and provide model vocabulary; assign result under the variable 'morph_matcher'\n",
    "morph_matcher = Matcher(vocab=nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a new pattern with rules for two *Tokens*:\n",
    "\n",
    " 1. Tokens that have a fine-grained part-of-speech tag `NNP` (proper noun), which can occur one or more times (operator: `+`)\n",
    " 2. Tokens that have a coarse part-of-speech tag `VERB` and have precisely the following morphological features (`MORPH`): `Number=Sing|Person=Three|Tense=Pres|VerbForm=Fin`\n",
    " \n",
    "We define the pattern using two dictionaries in a list, which we assign under the variable `propn_3rd_finite`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list with nested dictionaries that contains the pattern to be matched\n",
    "propn_3rd_finite = [{'TAG': 'NNP', 'OP': '+'},\n",
    "                    {'POS': 'VERB', 'MORPH': 'Number=Sing|Person=Three|Tense=Pres|VerbForm=Fin'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then add the pattern to the newly-created *Matcher* stored under the variable `morph_matcher` using the `add()` method.\n",
    "\n",
    "We also provide the value `LONGEST` to the optional argument `greedy` for the `add()` method.\n",
    "\n",
    "The `greedy` argument filters the matches for *Tokens* that include operators such as `+` that search *greedily* for more than one match.\n",
    "\n",
    "By setting the value to `LONGEST`, spaCy returns the longest sequence of matches instead of returning every match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the pattern to the matcher under the name 'sing_3rd_pres_fin'\n",
    "morph_matcher.add('sing_3rd_pres_fin', patterns=[propn_3rd_finite], greedy='LONGEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply the *Matcher* to the data stored under the variable `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Matcher to the Doc object under 'doc'; provide the argument 'as_spans'\n",
    "# and set its value to True to get Spans as output. Overwrite previous matches by\n",
    "# storing the result under the variable 'results'.\n",
    "morph_results = morph_matcher(doc, as_spans=True)\n",
    "\n",
    "# Loop over each Span object in the list 'morph_results'\n",
    "for result in morph_results:\n",
    "\n",
    "    # Print result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the matches are relatively few in number, because we defined that the verb should have quite specific morphological features.\n",
    "\n",
    "The question is, then, how can match just *some* morphological features?\n",
    "\n",
    "To loosen the criteria for morphological features by focusing on [tense](https://en.wikipedia.org/wiki/Grammatical_tense) only, we need to use a dictionary with the key `MORPH`, but instead of a string object, we provide a dictionary as the value.\n",
    "\n",
    "For this dictionary, we use the string `IS_SUPERSET` as the key. `IS_SUPERSET` is one of the attributes defined in the spaCy [pattern format](https://spacy.io/api/matcher#patterns).\n",
    "\n",
    "Before proceeding any further, let's unpack the logic behind `IS_SUPERSET` a bit: \n",
    "\n",
    "We can think of morphological features associated with a given Token as a [set](https://en.wikipedia.org/wiki/Set_(mathematics)). To exemplify, a set could consist of the following four items:\n",
    "\n",
    "```\n",
    "Number=Sing, Person=Three, Tense=Pres, VerbForm=Fin\n",
    "```\n",
    "\n",
    "If we would have *another set* with just one item, `Tense=Pres`, we could describe the relationship between the two sets by stating that the first set (with four items) is a superset of the second set (with one item).\n",
    "\n",
    "In other words, the larger (super)set contains the smaller (sub)set.\n",
    "\n",
    "This is also how matching using `IS_SUPERSET` works: spaCy retrieves the morphological features for a given *Token*, and examines whether these features are a superset of the features defined in the search pattern.\n",
    "\n",
    "The morphological features to be searched for are provided as a list of Python strings.\n",
    "\n",
    "These strings, in turn, define particular morphological features, e.g. `Tense=Past`, as defined in the [Universal Dependencies](https://universaldependencies.org/u/overview/morphology.html) schema for describing morphology.\n",
    "\n",
    "This list is then used as the value for the key `IS_SUPERSET`.\n",
    "\n",
    "Let's now proceed to search for verbs in the past tense and add them to the *Matcher* object under `morph_matcher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list with nested dictionaries that contains the pattern to be matched\n",
    "past_tense = [{'TAG': 'NNP', 'OP': '+'},\n",
    "              {'POS': 'VERB', 'MORPH': {'IS_SUPERSET': ['Tense=Past']}}]\n",
    "\n",
    "# Add the pattern to the matcher under the name 'past_tense'\n",
    "morph_matcher.add('past_tense', patterns=[past_tense], greedy='LONGEST')\n",
    "\n",
    "# Apply the Matcher to the Doc object under 'doc'; provide the argument 'as_spans'\n",
    "# and set its value to True to get Spans as output. Overwrite previous matches by\n",
    "# storing the result under the variable 'results'.\n",
    "morph_results = morph_matcher(doc, as_spans=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's loop over the results and print out the name of the pattern, the *Span* object containing the match, and the morphological features of the final *Token* in the match, which corresponds to the verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "# Loop over each Span object in the list 'results'\n",
    "for result in morph_results:\n",
    "    \n",
    "    # Print out the the name of the pattern rule, a tabulator character, and the matching Span.\n",
    "    # Finally, print another tabulator character, followed by the morphological features of the\n",
    "    # last Token in the match (a verb).\n",
    "    print(nlp.vocab[result.label].text, '\\t', result, '\\t', result[-1].morph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `past_tense` pattern can match objects based on a single morphological feature, although most matches share another morphological feature, namely a finite form of the verb. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining matches in context using concordances\n",
    "\n",
    "We can examine matches in their context of occurrence using *concordances*. In corpus linguistics, concordances are often understood as lines of text that show a match in its context of occurrence.\n",
    "\n",
    "These concordance lines can help the analyst to understand the context in which a particular token or structure occurs, and to develop further hypotheses.\n",
    "\n",
    "To create concordance lines using spaCy, let's start by importing the Printer class from wasabi, which is a small [Python library](https://pypi.org/project/wasabi/) that spaCy uses for colouring and formatting messages. We will use wasabi to highlight the matches in the concordance lines.\n",
    "\n",
    "We first initialise a *Printer* object, which we then assign under the variable `match`. Next, we test the *Printer* object by printing some text in red colour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Printer class from wasabi\n",
    "from wasabi import Printer\n",
    "\n",
    "# Initialise a Printer object; assign the object under the variable 'match'\n",
    "match = Printer()\n",
    "\n",
    "# Use the Printer to print out some text in red colour\n",
    "match.text(\"Hello world!\", color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to loop over the results returned by the *Matcher* object `morph_matcher`. As we learned above, the results consist of *Span* objects in a list, which are stored under the variable `morph_results`.\n",
    "\n",
    "We loop over items in this list and use the `enumerate()` function to keep track of their count. We also provide the argument `start` with the value 1 to the `enumerate()` function to start counting from the number 1.\n",
    "\n",
    "During the loop, we refer to this count using the variable `i` and to the *Span* object as `result`. The number under `i` is incremented with every *Span* object.\n",
    "\n",
    "We then print out the following output for each *Span* object in the list `morph_results`:\n",
    "\n",
    " 1. `i`: The number of the item in the list.\n",
    " 2. `doc[result.start - 7: result.start]`: A slice of the *Doc* object stored under the variable `doc`, which we searched for matches. As usual, we define a slice using brackets and separate the start and end of a slice using a colon. We take a slice that begins 7 *Tokens* before the start of the match (`result.start - 7`), and terminates at the start of the match `result.start`.\n",
    " 3. `match.text(result, color=\"red\", no_print=True)`: The matching *Span* object, rendered using the wasabi *Printer* object `match` in red colour. We also set the argument `no_print` to `True` to prevent wasabi from printing the output on a new line.\n",
    " 4. `doc[result.end: result.end + 7]`: Another slice of the *Doc* object stored under the variable `doc`. Here we take a slice that begins at the end of the match `result.end` and terminates 7 *Tokens* after the end of the match (`result.end + 7`).\n",
    " \n",
    "Essentially, we use the indices available under `start` and `end` attributes of each *Span* to retrieve the linguistic context in which the *Span* occurs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the matches in 'morph_results' and keep count of items\n",
    "for i, result in enumerate(morph_results, start=1):\n",
    "    \n",
    "    # Print following information for each match\n",
    "    print(i,   # Item number being looped over\n",
    "          doc[result.start - 7: result.start],   # The slice of the Doc preceding the match\n",
    "          match.text(result, color=\"red\", no_print=True),   # The match, rendered in red colour using wasabi\n",
    "          doc[result.end: result.end + 7])    # The slice of the Doc following the match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a set of concordance lines showing the matches in their context of occurrence.\n",
    "\n",
    "In some cases, the preceding or following *Tokens* consist of line breaks indicating a paragraph break, which causes the output to jump a row or two."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
