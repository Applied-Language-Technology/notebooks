{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with discourse-level annotations\n",
    "\n",
    "In the previous sections, we have applied various natural language processing techniques to plain text in order to create linguistic annotations for them.\n",
    "\n",
    "In this section, we turn towards using pre-existing linguistic annotations, focusing especially on annotations that target phenomena above the level of a clause.\n",
    "\n",
    "After reading through this section, you should:\n",
    "\n",
    " - know the basics of the CoNLL-U annotation schema\n",
    " - know how to create a spaCy *Doc* object manually\n",
    " - know how to annotate *Spans* in a *Doc* object using *SpanGroups*\n",
    " - know how to load CoNLL-U annotated corpora into spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the CoNLL-U annotation schema\n",
    "\n",
    "CoNLL-X is an annotation schema for describing linguistic features of diverse languages (Buchholz and Marsi [2006](https://www.aclweb.org/anthology/W06-2920)), which was originally developed to facilitate collaboration on so-called shared tasks in research on natural language processing (see e.g. Nissim et al. [2017](https://doi.org/10.1162/COLI_a_00304)).\n",
    "\n",
    "[CoNLL-U](https://universaldependencies.org/format.html) is a further development of this annotation schema for the Universal Dependencies formalism, which was introduced in [Part II](../notebooks/part_ii/03_basic_nlp.ipynb#Syntactic-parsing). This annotation schema is commonly used for distributing linguistic corpora in projects that build on the Universal Dependencies formalism.\n",
    "\n",
    "In addition to numerous modern languages, one can find, for example, CoNLL-U annotated corpora for ancient languages such Akkadian (Luukko et al. [2020](https://www.aclweb.org/anthology/2020.tlt-1.11)) and Coptic (Zeldes and Abrams [2018](https://www.aclweb.org/anthology/W18-6022))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The components of the CoNLL-U annotation schema\n",
    "\n",
    "CoNLL-U annotations are distributed as plain text files (see [Part II](http://localhost:8888/notebooks/part_ii/01_basic_text_processing.ipynb#Computers-and-text)).\n",
    "\n",
    "The annotation files contain three types of lines: **comment lines**, **word lines** and **blank lines**.\n",
    "\n",
    "**Comment lines** precede word lines and start with a hash character (#). These lines can be used to provide metadata about the sentence.\n",
    "\n",
    "Each **word line** contains annotations for a single word or token in a sentence.\n",
    "\n",
    "These annotations are provided using the following fields, separated by tabulator characters:\n",
    "\n",
    "```console\n",
    "ID\tFORM\tLEMMA\tUPOS\tXPOS\tFEATS\tHEAD\tDEPS\tMISC\n",
    "```\n",
    "\n",
    " 1. `ID`: Word index\n",
    " 2. `FORM`: The form of a word or punctuation symbol\n",
    " 3. `LEMMA`: Lemma or the base form of a word\n",
    " 4. `UPOS`: Universal part-of-speech tag\n",
    " 5. `XPOS`: Language-specific part-of-speech tag\n",
    " 6. `FEATS`: Morphological features\n",
    " 7. `HEAD`: Syntactic head of the current word\n",
    " 8. `DEPREL`: Universal dependency relation to the HEAD\n",
    " 9. `DEPS`: [Enhanced dependency relations](https://universaldependencies.org/u/overview/enhanced-syntax.html)\n",
    " 10. `MISC`: Any additional annotations\n",
    " \n",
    "Finally, a **blank line** is used to separate sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with CoNLL-U annotations in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore CoNLL-U annotations using Python, let's start by importing [conllu](https://github.com/EmilStenstrom/conllu/), a small library for parsing CoNLL-U annotations into data structures native to Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the conllu library\n",
    "import conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then open a plain text file with annotations in the CoNLL-U format from the [Georgetown Multilayer Corpus](https://corpling.uis.georgetown.edu/gum/) (GUM; see Zeldes [2017](http://dx.doi.org/10.1007/s10579-016-9343-x)), read its contents using the `read()` method and store the result under the variable `annotations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the plain text file for reading; assign under 'data'\n",
    "data = open(\"data/GUM_whow_parachute.conllu\", \"r\", encoding=\"utf-8\")\n",
    "\n",
    "# Read the file contents and assign under 'annotations'\n",
    "annotations = data.read()\n",
    "\n",
    "# Check the type of the resulting object\n",
    "type(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a Python string object. Let's print out the first 1000 characters of this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 1000 characters of the string under 'annotations'\n",
    "print(annotations[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the string object contains comment lines prefixed with a hash, followed by word lines with annotations for various fields. \n",
    "\n",
    "An underscore `_` is used to indicate fields with empty or missing values on the word lines. \n",
    "\n",
    "In the GUM corpus, the final field `MISC` contains values such as `Discourse` and `Entity` that provide annotations for discourse relations and entities such as events and objects.\n",
    "\n",
    "Here the question is: how to extract all this information programmatically from a *string* object?\n",
    "\n",
    "This is where the `conllu` module comes in handy, because its `parse()` function is capable of extracting information from CoNLL-U formatted strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the parse() function to parse the annotations; store under 'sentences'\n",
    "sentences = conllu.parse(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `parse()` function returns a Python list populated by *TokenList* objects. This object type is native to the conllu library.\n",
    "\n",
    "Let's examine the first item in the list `sentences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a *TokenList* object.\n",
    "\n",
    "To start with, the information contained in the comment lines in the CoNLL-U schema is provided under the `metadata` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metadata for the first item in the list\n",
    "sentences[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the GUM corpus uses the comment lines to provide four types of metadata for each sentence: `newdoc_id` for document identifier, `sent_id` for sentence identifier, `text` for plain text and `s_type` for sentence type or mood (Zeldes & Simonson [2017](https://www.aclweb.org/anthology/W16-1709): 69).\n",
    "\n",
    "Superficially, the object stored under the `metadata` attribute looks like a Python dictionary, but the object is actually a conllu *Metadata* object.\n",
    "\n",
    "This object, however, behaves just like a Python dictionary in the sense that it consists of key and value pairs, which are accessed just like those in a dictionary.\n",
    "\n",
    "To exemplify, to retrieve the sentence type (or mood), simply use the key `s_type` to access the *Metadata* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sentence type under 's_type'\n",
    "sentences[0].metadata['s_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns the string `inf`, which corresponds to infinitive.\n",
    "\n",
    "Coming back to the *TokenList* object, as the name suggest, the items in a *TokenList* consist of individual *Token* objects.\n",
    "\n",
    "Let's access the first *Token* object `[0]` in the first *TokenList* object `[0]` under `sentences`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first token in the first sentence\n",
    "sentences[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the *TokenList* above, the *Token* object is a dictionary-like object with keys and values.\n",
    "\n",
    "The dictionary under the key `misc` holds information about discourse relations, which describe how parts of a text relate to each other using [Rhetorical Structure Theory](https://www.sfu.ca/rst) (Mann & Thompson [1988](https://doi.org/10.1515/text.1.1988.8.3.243)).\n",
    "\n",
    "In this case, the annotation states that a relation named **preparation** holds between units 1 and 11.\n",
    "\n",
    "These units correspond to *elementary discourse units* instead of words or sentences in the document. \n",
    "\n",
    "As such, they define an additional level of *segmentation*, which seeks to capture units of discourse that are placed in various relations to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding discourse-level annotations to *Doc* objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a variable with value 0 that we will use for counting\n",
    "# the Tokens that we process\n",
    "counter = 0\n",
    "\n",
    "# Set up placeholder lists for the information that we will extract\n",
    "# from the CoNLL-U annotations. These lists will be used to create \n",
    "# a spaCy Doc object below.\n",
    "words = []\n",
    "spaces = []\n",
    "sent_starts = []\n",
    "\n",
    "# We use these lists to keep track of sentences, discourse units\n",
    "# and the relations that hold between them.\n",
    "discourse_units = []\n",
    "sent_types = []\n",
    "relations = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the value stored under the variable `counter` to keep track of the boundaries for sentences and elementary discourse units as we loop over the *TokenList* objects stored in the list `sentences`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each TokenList object\n",
    "for sentence in sentences:\n",
    "    \n",
    "    # When we begin looping over a new sentence, set the value of\n",
    "    # the variable 'is_start' to True.\n",
    "    is_start = True\n",
    "    \n",
    "    # Add the sentence type to the list 'sent_types'\n",
    "    sent_types.append(sentence.metadata['s_type'])\n",
    "        \n",
    "    # Proceed to loop over the Tokens in the TokenList object\n",
    "    for token in sentence:\n",
    "        \n",
    "        # Use the key 'form' to retrieve the annotations for the \n",
    "        # Token and append it to the placeholder list.\n",
    "        words.append(token['form'])\n",
    "        \n",
    "        # Check if this Token begins a sentence by evaluating whether\n",
    "        # the variable 'is_start' is True.\n",
    "        if is_start:\n",
    "            \n",
    "            # If the Token starts a sentence, add value True to the list\n",
    "            # 'sent_starts'.\n",
    "            sent_starts.append(True)\n",
    "            \n",
    "            # Set the variable 'is_start' to False until the next sentence\n",
    "            # starts and the variable is set to True again.\n",
    "            is_start = False\n",
    "        \n",
    "        # If the variable 'is_start' is False\n",
    "        else:\n",
    "            \n",
    "            # Append value 'False' to the list 'sent_starts'\n",
    "            sent_starts.append(False)\n",
    "        \n",
    "        # Check if the key 'misc' contains anything, and if the key\n",
    "        # holds the value 'Discourse', proceed to the code block below\n",
    "        if token['misc'] is not None and 'Discourse' in token['misc']:\n",
    "            \n",
    "            # The presence of the key 'Discourse' indicates the beginning\n",
    "            # of a new elementary discourse unit; add its index to the list\n",
    "            # 'discourse_units'.\n",
    "            discourse_units.append(counter)\n",
    "            \n",
    "            # Unpack the relationship definition; start by splitting the\n",
    "            # relation name from the elementary discourse units. Assign\n",
    "            # the resulting objects under 'relation' and 'edus'.\n",
    "            relation, edus = token['misc']['Discourse'].split(':')\n",
    "            \n",
    "            # Try to split the relation annotation into two parts\n",
    "            try:\n",
    "                \n",
    "                # Split at the '->' string and assign to 'source'\n",
    "                # and 'target', respectively.\n",
    "                source, target = edus.split('->')\n",
    "                \n",
    "                # Deduct -1 from both 'source' and 'target', because \n",
    "                # the identifiers used in the GUM corpus are not \n",
    "                # zero-indexed, but spaCy spans that correspond to\n",
    "                # elementary discourse units will be. Also cast the\n",
    "                # numbers into integers.\n",
    "                source, target = int(source) - 1, int(target) - 1\n",
    "            \n",
    "            # The root node of the RST tree will not have a target,\n",
    "            # which raises a ValueError since there is only one item.\n",
    "            except ValueError:\n",
    "                \n",
    "                # Assign the first item in 'edus' to 'source' and set\n",
    "                # target to None. \n",
    "                source, target = edus[0], None\n",
    "                \n",
    "                # Deduct -1 from 'source' as explained above.\n",
    "                source = int(source) - 1 \n",
    "                \n",
    "            # Compile the relation definition into a three tuple and\n",
    "            # append to the list 'relations'.\n",
    "            relations.append((relation, source, target))\n",
    "            \n",
    "        # Check if the current Token is followed by a whitespace. If this is\n",
    "        # not the case, e.g. for the Token at the end of a TokenList, this\n",
    "        # information is available under the 'misc' key.\n",
    "        if token['misc'] is not None and 'SpaceAfter' in token['misc']:\n",
    "            \n",
    "            # If the 'misc' key holds a dictionary with the key 'SpaceAfter'\n",
    "            # with a value 'No', proceed below\n",
    "            if token['misc']['SpaceAfter'] == 'No':\n",
    "                \n",
    "                # Append the Boolean value 'False' to the list 'spaces'.\n",
    "                # Note the missing quotation marks: this is a Boolean value\n",
    "                # (True / False)\n",
    "                spaces.append(False)\n",
    "            \n",
    "        # If the 'SpaceAfter' key is not found under 'misc', the token is followed\n",
    "        # by a space.\n",
    "        else:\n",
    "\n",
    "            # Append True to the list of spaces\n",
    "            spaces.append(True)\n",
    "        \n",
    "        # Update the counter as we finish looping over a Token object\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This collects the information needed for creating a spaCy *Doc* object, together with the discourse-level annotations that we add to the *Doc* object afterwards.\n",
    "\n",
    "Typically, we would create a spaCy *Doc* object by passing some text to a *Language* object, as shown in [Part II](../part_ii/03_basic_nlp.ipynb#Performing-basic-NLP-tasks-using-spaCy).\n",
    "\n",
    "In this case, however, we need to preserve the tokens defined in the CoNLL-U annotations, because this information is needed to align the discourse-level annotations correctly for both sentences and elementary discourse units.\n",
    "\n",
    "In other words, we cannot take the risk that spaCy tokenises the text differently, because this would result in misaligned annotations for sentences and elementary discourse units.\n",
    "\n",
    "Hence we create a spaCy *Doc* object manually by importing the *Doc* class. We also load a small language model for English and store it under the variable `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Doc class and the spaCy library\n",
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "\n",
    "# Load a small language model for English; store under 'nlp'\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the *Doc* class to create a *Doc* object manually by providing the information in the list `words`, `spaces` and `sent_starts` that we just created as input.\n",
    "\n",
    "In addition, we must pass a *Vocabulary* object to the `vocab` argument to associate the *Doc* with a given language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spaCy Doc object \"manually\"; assign under the variable 'doc'\n",
    "doc = Doc(vocab=nlp.vocab, \n",
    "          words=words, \n",
    "          spaces=spaces,\n",
    "          sent_starts=sent_starts\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a spaCy *Doc* object with *Tokens* and sentence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Tokens up to index 8 from the Doc object\n",
    "doc[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, spaCy has successfully assigned the input data to *Token* objects.\n",
    "\n",
    "The sentence boundaries, in turn, are used to define the sentences under the attribute `sents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the first five sentences in the Doc object\n",
    "list(doc.sents)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, because we discarded the linguistic information contained in the CoNLL-U annotations, the attributes of the *Token* objects in our *Doc* object are empty.\n",
    "\n",
    "Let's fetch the fine-grained part-of-speech tag for the first *Token* in the *Doc* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the fine-grained part-of-speech tag for Token at index 0\n",
    "doc[0].tag_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this shows, the `tag_` attribute is empty.\n",
    "\n",
    "We can create these annotations afterwards by passing the *Doc* object stored under `doc` to various components of the *Language* object.\n",
    "\n",
    "These components are accessible under the attribute `pipeline`, as we learned in [Part II](../part_ii/04_basic_nlp_continued.ipynb#Modifying-spaCy-pipelines).\n",
    "\n",
    "Let's loop over the components of the `pipeline` and apply them to the *Doc* object under `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the name / component pairs under the 'pipeline' attribute\n",
    "# of the Language object 'nlp'.\n",
    "for name, component in nlp.pipeline:\n",
    "    \n",
    "    # Use a formatted string to print out the 'name' of the component\n",
    "    print(f\"Now applying component {name} ...\")\n",
    "    \n",
    "    # Feed the existing Doc object to the component and store the updated\n",
    "    # annotations under the variable of the same name ('doc').\n",
    "    doc = component(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now examine the attribute `tag_` of the first *Token* object, the fine-grained part-of-speech tag has been added to the *Token*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the fine-grained part-of-speech tag for Token at index 0\n",
    "doc[0].tag_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we have access to additional linguistic annotations produced by spaCy, such as noun phrases under the attribute `noun_chunk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first five noun phrases in the Doc object\n",
    "list(doc.noun_chunks)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our manually-defined sentence boundaries, however, remain the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first five sentences in the Doc object\n",
    "list(doc.sents)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding information on sentence mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens.span_group import SpanGroup\n",
    "from spacy.tokens import Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Span.set_extension('mood', default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_group = SpanGroup(doc=doc, name=\"sentences\", spans=list(doc.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.spans['sentences'] = sent_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mood, span in zip(sent_types, doc.spans['sentences']):\n",
    "    \n",
    "    span._.mood = mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.spans['sentences'][4]._.mood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding information on discourse relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a placeholder list to hold slices of the Doc object that correspond\n",
    "# to discourse units.\n",
    "edu_spans = []\n",
    "\n",
    "# Proceed to loop over discourse unit boundaries using Python's range() function.\n",
    "# This will give us numbers, which we use to index the 'discourse_units' list that\n",
    "# contains the indices that mark the beginning of a discourse unit.\n",
    "for i in range(len(discourse_units)):\n",
    "    \n",
    "    # Try to execute the following code block\n",
    "    try:\n",
    "        \n",
    "        # Get the current item in the list 'discourse_units' and the next item; assign\n",
    "        # under variables 'start' and 'end'.\n",
    "        start, end = discourse_units[i], discourse_units[i + 1]\n",
    "    \n",
    "    # If the next item is not available, because we've reached the final item in the list,\n",
    "    # this will raise an IndexError that we catch here.\n",
    "    except IndexError:\n",
    "        \n",
    "        # Assign the start of the discourse unit as usual, set the length of the Doc \n",
    "        # object as the value for 'end' to mark the end point of the discourse unit. \n",
    "        start, end = discourse_units[i], len(doc)\n",
    "\n",
    "    # Use the 'start' and 'end' variables to slice the Doc object; append the\n",
    "    # resulting Span object to the list 'edu_spans'.\n",
    "    edu_spans.append(doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first five Spans in the list 'edu_spans'\n",
    "edu_spans[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register three custom attributes for Span objects, which correspond to\n",
    "# elementary discourse unit id, the id of the element acting as the target,\n",
    "# and the name of the relation.\n",
    "Span.set_extension('edu_id', default=None)\n",
    "Span.set_extension('target_id', default=None)\n",
    "Span.set_extension('relation', default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SpanGroup object from the Spans in the 'edu_spans' list\n",
    "edu_group = SpanGroup(doc=doc, name=\"edus\", spans=edu_spans)\n",
    "\n",
    "# Assign the SpanGroup under the key 'edus'\n",
    "doc.spans['edus'] = edu_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for relation in relations:\n",
    "    \n",
    "    rel_name, source, target = relation[0], relation[1], relation[2]\n",
    "    \n",
    "    doc.spans['edus'][source]._.edu_id = source\n",
    "    doc.spans['edus'][source]._.target_id = target\n",
    "    doc.spans['edus'][source]._.relation = rel_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.spans['edus'][5]._.relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting CoNLL-U annotations into *Doc* objects\n",
    "\n",
    "If you do not need to enrich spaCy objects with additional information, but simply wish to convert CoNLL-U annotations into *Doc* objects, spaCy provides a convenience function, `conllu_to_docs()`, for converting CoNLL-U annotated data into spacy *Doc* objects.\n",
    "\n",
    "Let's start by importing the function from the `training` submodule, as this function is mainly used for loading CoNLL-U annotated data for training language models. We also import the class for the *Doc* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'conllu_to_docs' function and the Doc class\n",
    "from spacy.training.converters import conllu_to_docs\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `conllu_to_docs()` function takes a Python string object as input.\n",
    "\n",
    "We pass the string object `annotations` that contains CoNLL-U annotations to the function, and set the argument `no_print` to `True` to prevent the `conllu_to_docs()` function from printing status messages.\n",
    "\n",
    "The function returns a Python generator object, which we must cast into a list to examine its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the string object under 'annotations' to the 'conllu_to_docs' function. \n",
    "# Set 'no_print' to True and cast the result into a Python list; store under 'docs'.\n",
    "docs = list(conllu_to_docs(annotations, no_print=True))\n",
    "\n",
    "# Get the first two items in the resulting list\n",
    "docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a list with *Doc* objects. By default, the `conllu_to_docs()` function groups every ten sentences in the CoNLL-u files into a single spaCy object. \n",
    "\n",
    "This, however, is not an optimal solution, as having every document its own *Doc* object would make more sense rather than an arbitrary grouping.\n",
    "\n",
    "To do so, we can use the `from_docs()` method of the *Doc* object to combine the *Doc* objects in the list `docs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Doc objects in the list 'docs' into a single Doc; assign under 'doc'\n",
    "doc = Doc.from_docs(docs)\n",
    "\n",
    "# Check variable type and length\n",
    "type(doc), len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a single spaCy *Doc* object with 890 *Tokens*.\n",
    "\n",
    "If we loop over the first eight *Tokens* in the *Doc* object `doc` and print out their linguistic annotations, the results shows that the information from the CoNLL-U annotations have been carried over to the *Doc* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the first 8 Tokens using the range() function\n",
    "for token_ix in range(0, 8):\n",
    "    \n",
    "    # Use the current number under 'token_ix' to fetch a Token from the Doc.\n",
    "    # Assign the Token object under the variable 'token'. \n",
    "    token = doc[token_ix]\n",
    "    \n",
    "    # Print the Token and its linguistic annotations\n",
    "    print(token, token.tag_, token.pos_, token.morph, token.dep_, token.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we attempt to retrieve the noun phrases in the *Doc* objects available under the attribute `noun_chunks`, spaCy will return an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This raises an error, because the *Doc* that we created using the `conllu_to_docs()` function does not have a *Language* and a *Vocabulary* associated with it.\n",
    "\n",
    "The noun phrases are created using language-specific rules from syntactic parses, but spaCy does not know which language it is working with.\n",
    "\n",
    "Because the language of a *Doc* cannot be defined manually, we must use a trick involving the *DocBin* object that we learned about in [Part II](../part_ii/04_basic_nlp_continued.ipynb#Writing-processed-texts-to-disk).\n",
    "\n",
    "The *DocBin* is a special object type for writing spaCy annotations to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DocBin object from the 'tokens' submodule\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# Create an empty DocBin object\n",
    "doc_bin = DocBin()\n",
    "\n",
    "# Add the current Doc to the DocBin\n",
    "doc_bin.add(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of writing the *DocBin* object to disk, we simply retrieve the *Doc* objects from the *DocBin* using the `get_docs()` method, which requires a *Vocabulary* object as input to the `vocab` argument.\n",
    "\n",
    "The *Vocabulary* is used to associate the *Doc* objects with a given language.\n",
    "\n",
    "The `get_docs()` method returns a generator, which we must cast into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 'get_docs' method to retrieve the Docs from the DocBin\n",
    "docs = list(doc_bin.get_docs(vocab=nlp.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now examine the *Doc* object, which is naturally the first and only item in the list, and retrieve its attribute `noun_chunks`, we can also get the noun phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(docs[0].noun_chunks)[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
