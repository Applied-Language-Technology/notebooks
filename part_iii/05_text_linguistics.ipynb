{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with discourse-level annotations\n",
    "\n",
    "So far, we have applied various natural language processing techniques to plain text, such as part-of-speech tagging, parsing syntactic dependencies and so on.\n",
    "\n",
    "In this section, we turn towards pre-existing linguistic annotations, focusing especially on annotations that target phenomena above the level of a clause.\n",
    "\n",
    "After reading through this section, you should:\n",
    "\n",
    " - understand the CoNLL-U annotation schema\n",
    " - know how to load CoNLL-U annotated corpora into spaCy\n",
    " - visualise discourse structures using spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the CoNLL-U annotation schema\n",
    "\n",
    "CoNLL-X is an annotation schema for describing linguistic features of diverse languages, which was originally developed to facilitate collaboration on so-called shared tasks (see e.g. Nissim et al. [2017](https://doi.org/10.1162/COLI_a_00304)) in research on natural language processing (Buchholz and Marsi [2006](https://www.aclweb.org/anthology/W06-2920)).\n",
    "\n",
    "[CoNLL-U](https://universaldependencies.org/format.html) is a further development of this annotation schema for the Universal Dependencies formalism, which was introduced in [Part II](../notebooks/part_ii/03_basic_nlp.ipynb#Syntactic-parsing).\n",
    "\n",
    "The CoNLL-U annotation schema is used for distributing linguistic corpora in projects that build on the Universal Dependencies formalism.\n",
    "\n",
    "One can find, for example, CoNLL-U annotated corpora for ancient languages such Akkadian (Luukko et al. [2020](https://www.aclweb.org/anthology/2020.tlt-1.11)) and Coptic (Zeldes and Abrams [2018](https://www.aclweb.org/anthology/W18-6022))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The components of the CoNLL-U annotation schema\n",
    "\n",
    "CoNLL-U annotations are distributed as plain text files (see [Part II](http://localhost:8888/notebooks/part_ii/01_basic_text_processing.ipynb#Computers-and-text)).\n",
    "\n",
    "The annotation files contain three types of lines: **comment lines**, **word lines** and **blank lines**.\n",
    "\n",
    "**Comment lines** precede word lines and start with a hash character (#). These lines can be used to provide metadata about the sentence.\n",
    "\n",
    "Each **word line** contains annotations for a single word or token.\n",
    "\n",
    "These annotations are provided using the following fields, separated by tabulator characters:\n",
    "\n",
    "```console\n",
    "ID\tFORM\tLEMMA\tUPOS\tXPOS\tFEATS\tHEAD\tDEPS\tMISC\n",
    "```\n",
    "\n",
    " 1. `ID`: Word index\n",
    " 2. `FORM`: The form of a word or punctuation symbol\n",
    " 3. `LEMMA`: Lemma or the base form of a word\n",
    " 4. `UPOS`: Universal part-of-speech tag\n",
    " 5. `XPOS`: Language-specific part-of-speech tag\n",
    " 6. `FEATS`: Morphological features\n",
    " 7. `HEAD`: Syntactic head of the current word\n",
    " 8. `DEPREL`: Universal dependency relation to the HEAD\n",
    " 9. `DEPS`:\n",
    " 10. `MISC`: Any additional annotations\n",
    " \n",
    "Finally, a **blank line** is used to separate sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with CoNLL-U annotations in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing [conllu](https://github.com/EmilStenstrom/conllu/), a small Python library for parsing CoNLL-U annotations into data structures native to Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the conllu library\n",
    "import conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then open a plain text file with annotations in the CoNLL-U format from the [Georgetown Multilayer Corpus](https://corpling.uis.georgetown.edu/gum/) (GUM; see Zeldes [2017](http://dx.doi.org/10.1007/s10579-016-9343-x)), read its contents using the `read()` method and store the result under the variable `annotations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the plain text file for reading; assign under 'data'\n",
    "data = open(\"data/GUM_whow_parachute.conllu\", \"r\", encoding=\"utf-8\")\n",
    "\n",
    "# Read the file contents and assign under 'annotations'\n",
    "annotations = data.read()\n",
    "\n",
    "# Check the type of the resulting object\n",
    "type(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a Python string object. Let's print out the first 1000 characters of this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 1000 characters of the string under 'annotations'\n",
    "print(annotations[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the string object contains comment lines prefixed with a hash, followed by word lines with annotations for various fields. \n",
    "\n",
    "An underscore `_` is used to indicate fields with empty or missing values on the word lines. \n",
    "\n",
    "In the GUM corpus, the final field `MISC` contains values such as `Discourse` and `Entity` that provide annotations for discourse relations and entities such as events and objects.\n",
    "\n",
    "Here the question is: how to extract all this information programmatically from a *string* object?\n",
    "\n",
    "This is where the `conllu` module comes in handy, because its `parse()` function is capable of extracting information from CoNLL-U formatted strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the parse() function to parse the annotations; store under 'sentences'\n",
    "sentences = conllu.parse(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `parse()` function returns a Python list populated by *TokenList* objects. This object type is native to the conllu library.\n",
    "\n",
    "Let's examine the first item in the list `sentences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a *TokenList* object.\n",
    "\n",
    "To start with, the information provided using comment lines in the CoNLL-U schema is provided under the `metadata` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metadata for the first item in the list\n",
    "sentences[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the GUM corpus contains four types of metadata for each sentence: `newdoc_id` for document identifier, `sent_id` for sentence identifier, `text` for plain text and `s_type` for sentence type or mood (Zeldes & Simonson [2017](https://www.aclweb.org/anthology/W16-1709): 69).\n",
    "\n",
    "Superficially, the object stored under the `metadata` attribute looks like a Python dictionary, but is actually a conllu *Metadata* object.\n",
    "\n",
    "This object, however, behaves just like a Python dictionary in the sense that it consists of key and value pairs, which are accessed just like those in a Python dictionary.\n",
    "\n",
    "To exemplify, to retrieve the sentence type (or mood), simply use the key `s_type` to access the *Metadata* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sentence type under 's_type'\n",
    "sentences[0].metadata['s_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns the string `inf`, which corresponds to infinitive.\n",
    "\n",
    "Coming back to the *TokenList* object, as the name suggest, the items in a *TokenList* consist of individual *Token* objects.\n",
    "\n",
    "Let's access the first *Token* object `[0]` in the first *TokenList* object `[0]` under `sentences`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first token in the first sentence\n",
    "sentences[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the *TokenList* above, the *Token* object is a dictionary-like object with keys and values.\n",
    "\n",
    "As you can see, the dictionary under the key `misc` holds information about discourse relations, which explicate how the pieces of text relate to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "discourse_units = []\n",
    "relations = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    \n",
    "    for token in sentence:\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        if token['misc'] is not None and 'Discourse' in token['misc']:\n",
    "            \n",
    "            discourse_units.append(counter)\n",
    "            \n",
    "            relation, edus = token['misc']['Discourse'].split(':')\n",
    "                        \n",
    "    discourse_units.append(counter)\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting CoNLL-U annotations into spaCy *Docs*\n",
    "\n",
    "spaCy provides a convenience function, `conllu_to_docs()`, which allows converting CoNLL-U annotated data into spacy *Doc* objects.\n",
    "\n",
    "Let's start by importing the function from the `training` submodule. We also import the class for the *Doc* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'conllu_to_docs' function and the Doc class\n",
    "from spacy.training.converters import conllu_to_docs\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `conllu_to_docs()` function takes a Python string object as input.\n",
    "\n",
    "We pass the string object `annotations` that contains CoNLL-U annotations to the function, and set the argument `no_print` to `True` to prevent the `conllu_to_docs()` function from printing status messages.\n",
    "\n",
    "The function returns a Python generator object, which means we must cast the output into a list to examine its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the string object under 'annotations' to the 'conllu_to_docs' function. \n",
    "# Set 'no_print' to True and cast the result into a Python list; store under 'docs'.\n",
    "docs = list(conllu_to_docs(annotations, no_print=True))\n",
    "\n",
    "# Get the first two items in the resulting list\n",
    "docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a list with *Doc* objects. \n",
    "\n",
    "By default, the `conllu_to_docs()` function groups every ten sentences in the CoNLL-u files into a single spaCy object. \n",
    "\n",
    "This, however, is not an optimal solution, as having every document its own *Doc* object would make more sense rather than an arbitrary grouping.\n",
    "\n",
    "To fix this, we can use the `from_docs()` method of the *Doc* object to combine the *Doc* objects in the list `docs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Doc objects in the list 'docs' into a single Doc; assign under 'doc'\n",
    "doc = Doc.from_docs(docs)\n",
    "\n",
    "# Check variable type and length\n",
    "type(doc), len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a single spaCy *Doc* object with 890 *Tokens*.\n",
    "\n",
    "If we loop over the first eight *Tokens* in the *Doc* object `doc` and print out their linguistic annotations, the results shows that the information from the CoNLL-U annotations have been carried over to the *Doc* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the first 8 Tokens using the range() function\n",
    "for token_ix in range(0, 8):\n",
    "    \n",
    "    # Use the current number under 'token_ix' to fetch a Token from the Doc.\n",
    "    # Assign the Token object under the variable 'token'. \n",
    "    token = doc[token_ix]\n",
    "    \n",
    "    # Print the Token and its linguistic annotations\n",
    "    print(token, token.tag_, token.pos_, token.morph, token.dep_, token.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Make a note about the missing properties, e.g. (`noun_chunks`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
